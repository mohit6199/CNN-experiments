{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionNet_v1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohit6199/CNN-experiments/blob/master/InceptionNet_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr4OXRdrqxrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZx2xuuyrRgT",
        "colab_type": "code",
        "outputId": "544e9d9e-d7c0-42bb-ed47-edc97ef54872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZU4ZBRIrSAF",
        "colab_type": "code",
        "outputId": "db30eea4-dfb3-41e1-fff7-b0de3fc86da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers.core import Layer\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnN28Mknrxwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPool2D,  \\\n",
        "    Dropout, Dense, Input, concatenate,      \\\n",
        "    GlobalAveragePooling2D, AveragePooling2D,\\\n",
        "    Flatten\n",
        "\n",
        "import cv2 \n",
        "import numpy as np \n",
        "from keras.datasets import cifar10 \n",
        "from keras import backend as K \n",
        "from keras.utils import np_utils\n",
        "\n",
        "import math \n",
        "from keras.optimizers import SGD \n",
        "from keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jlmpSKGr2QK",
        "colab_type": "code",
        "outputId": "ab759e10-5429-49fd-8534-f0541fb95aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "num_classes = 10\n",
        "\n",
        "def load_cifar10_data(img_rows, img_cols):\n",
        "\n",
        "    # Load cifar10 training and validation sets\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    index_train=[]\n",
        "    for i in range(0,50000):\n",
        "        if y_train[i]>5:\n",
        "            index_train.append(i)\n",
        "    x_train=np.delete(x_train,index_train,0)\n",
        "    y_train=np.delete(y_train,index_train,0)\n",
        "\n",
        "    print('x_train ' , x_train.shape)\n",
        "    print('y_train ', y_train.shape)\n",
        "\n",
        "    index_test=[]\n",
        "    for i in range(0,10000):\n",
        "        if y_test[i]>5:\n",
        "            index_test.append(i)\n",
        "    x_test=np.delete(x_test,index_test,0)\n",
        "    y_test=np.delete(y_test,index_test,0)\n",
        "\n",
        "    print('x_test ' , x_test.shape)\n",
        "    print('y_test ', y_test.shape)\n",
        "    #--------------------------------------------------------------------------\n",
        "    ith_class={}\n",
        "    for i in np.arange(0,6):\n",
        "        ith_class[i]=[]\n",
        "        \n",
        "    ith_class_test={}\n",
        "    for i in np.arange(0,6):\n",
        "        ith_class_test[i]=[]\n",
        "        \n",
        "    for i in range(0,30000):\n",
        "        ith_class[y_train[i][0]].append(i)\n",
        "    for i in range(0,6000):\n",
        "        ith_class_test[y_test[i][0]].append(i)\n",
        "\n",
        "    delete_list=[]\n",
        "    for i in np.arange(0,6):\n",
        "        delete_list.append(ith_class[i][500:5000])\n",
        "    x_train=np.delete(x_train,delete_list,0)\n",
        "    y_train=np.delete(y_train,delete_list,0)\n",
        "\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "\n",
        "    delete_list_val=[]\n",
        "    for i in np.arange(0,6):\n",
        "        delete_list_val.append(ith_class_test[i][100:1000])\n",
        "    x_test=np.delete(x_test,delete_list_val,0)\n",
        "    y_test=np.delete(y_test,delete_list_val,0)\n",
        "\n",
        "    print(x_test.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    # Resize training images\n",
        "    x_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in x_train[:,:,:,:]])\n",
        "    x_test = np.array([cv2.resize(img, (img_rows,img_cols)) for img in x_test[:,:,:,:]])\n",
        "\n",
        "    # Transform targets to keras compatible format\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    \n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    # preprocess data\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "\n",
        "    print(x_train.shape)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_cifar10_data(224, 224)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train  (30000, 32, 32, 3)\n",
            "y_train  (30000, 1)\n",
            "x_test  (6000, 32, 32, 3)\n",
            "y_test  (6000, 1)\n",
            "(3000, 32, 32, 3)\n",
            "(3000, 1)\n",
            "(600, 32, 32, 3)\n",
            "(600, 1)\n",
            "(3000, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV-uzFtjs1BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception_module(x,\n",
        "                     filters_1x1,\n",
        "                     filters_3x3_reduce,\n",
        "                     filters_3x3,\n",
        "                     filters_5x5_reduce,\n",
        "                     filters_5x5,\n",
        "                     filters_pool_proj,\n",
        "                     name=None):\n",
        "    \n",
        "    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    \n",
        "    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU49x_yQuO4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_init = keras.initializers.glorot_uniform()\n",
        "bias_init = keras.initializers.Constant(value=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HSAgXiSuRi6",
        "colab_type": "code",
        "outputId": "f34ff3c2-862a-449c-aec8-5348038c269d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "input_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)\n",
        "x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)\n",
        "x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=64,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=128,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=32,\n",
        "                     filters_pool_proj=32,\n",
        "                     name='inception_3a')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=192,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_3b')\n",
        "\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=192,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=208,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=48,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4a')\n",
        "\n",
        "\n",
        "x1 = AveragePooling2D((5, 5), strides=3)(x)\n",
        "x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n",
        "x1 = Flatten()(x1)\n",
        "x1 = Dense(1024, activation='relu')(x1)\n",
        "x1 = Dropout(0.7)(x1)\n",
        "x1 = Dense(10, activation='softmax', name='auxilliary_output_1')(x1)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=160,\n",
        "                     filters_3x3_reduce=112,\n",
        "                     filters_3x3=224,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4b')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=256,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4c')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=112,\n",
        "                     filters_3x3_reduce=144,\n",
        "                     filters_3x3=288,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4d')\n",
        "\n",
        "\n",
        "x2 = AveragePooling2D((5, 5), strides=3)(x)\n",
        "x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n",
        "x2 = Flatten()(x2)\n",
        "x2 = Dense(1024, activation='relu')(x2)\n",
        "x2 = Dropout(0.7)(x2)\n",
        "x2 = Dense(10, activation='softmax', name='auxilliary_output_2')(x2)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_4e')\n",
        "\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5a')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=384,\n",
        "                     filters_3x3_reduce=192,\n",
        "                     filters_3x3=384,\n",
        "                     filters_5x5_reduce=48,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5b')\n",
        "\n",
        "x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\n",
        "\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "x = Dense(10, activation='softmax', name='output')(x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1aZdvW5ucBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(input_layer, [x, x1, x2], name='inception_v1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUvka_Fhuhxl",
        "colab_type": "code",
        "outputId": "cece5368-08da-4224-f1f2-e559fb6aaaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1_7x7/2 (Conv2D)           (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_1_3x3/2 (MaxPooling2D) (None, 56, 56, 64)   0           conv_1_7x7/2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a_3x3/1 (Conv2D)          (None, 56, 56, 64)   4160        max_pool_1_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b_3x3/1 (Conv2D)          (None, 56, 56, 192)  110784      conv_2a_3x3/1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_2_3x3/2 (MaxPooling2D) (None, 28, 28, 192)  0           conv_2b_3x3/1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 28, 28, 96)   18528       max_pool_2_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 28, 28, 16)   3088        max_pool_2_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 192)  0           max_pool_2_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 28, 28, 64)   12352       max_pool_2_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 28, 28, 128)  110720      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 32)   12832       conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 28, 28, 32)   6176        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_3a (Concatenate)      (None, 28, 28, 256)  0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 28, 28, 128)  32896       inception_3a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 28, 28, 32)   8224        inception_3a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 256)  0           inception_3a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 28, 28, 128)  32896       inception_3a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 28, 28, 192)  221376      conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 28, 28, 96)   76896       conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 28, 28, 64)   16448       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_3b (Concatenate)      (None, 28, 28, 480)  0           conv2d_7[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_3_3x3/2 (MaxPooling2D) (None, 14, 14, 480)  0           inception_3b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 14, 14, 96)   46176       max_pool_3_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 14, 14, 16)   7696        max_pool_3_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 480)  0           max_pool_3_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 14, 14, 192)  92352       max_pool_3_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 14, 14, 208)  179920      conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 14, 14, 48)   19248       conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 14, 14, 64)   30784       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_4a (Concatenate)      (None, 14, 14, 512)  0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 14, 14, 112)  57456       inception_4a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 14, 14, 24)   12312       inception_4a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 512)  0           inception_4a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 14, 14, 160)  82080       inception_4a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 14, 14, 224)  226016      conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 14, 14, 64)   38464       conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_4b (Concatenate)      (None, 14, 14, 512)  0           conv2d_20[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 14, 14, 128)  65664       inception_4b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 14, 14, 24)   12312       inception_4b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 512)  0           inception_4b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 14, 14, 128)  65664       inception_4b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 14, 14, 256)  295168      conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 14, 14, 64)   38464       conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_4c (Concatenate)      (None, 14, 14, 512)  0           conv2d_26[0][0]                  \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 14, 14, 144)  73872       inception_4c[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 14, 14, 32)   16416       inception_4c[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 512)  0           inception_4c[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 14, 14, 112)  57456       inception_4c[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 14, 14, 288)  373536      conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 14, 14, 64)   51264       conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_4d (Concatenate)      (None, 14, 14, 528)  0           conv2d_32[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 14, 14, 160)  84640       inception_4d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 14, 14, 32)   16928       inception_4d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 528)  0           inception_4d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 14, 14, 256)  135424      inception_4d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 14, 14, 320)  461120      conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 14, 14, 128)  102528      conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 14, 14, 128)  67712       max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_4e (Concatenate)      (None, 14, 14, 832)  0           conv2d_39[0][0]                  \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_4_3x3/2 (MaxPooling2D) (None, 7, 7, 832)    0           inception_4e[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 7, 7, 160)    133280      max_pool_4_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 7, 7, 32)     26656       max_pool_4_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 832)    0           max_pool_4_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 7, 7, 256)    213248      max_pool_4_3x3/2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 7, 7, 320)    461120      conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 7, 7, 128)    102528      conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 7, 7, 128)    106624      max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inception_5a (Concatenate)      (None, 7, 7, 832)    0           conv2d_45[0][0]                  \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 7, 7, 192)    159936      inception_5a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 7, 7, 48)     39984       inception_5a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 7, 7, 832)    0           inception_5a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 4, 4, 512)    0           inception_4a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 528)    0           inception_4d[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 7, 7, 384)    319872      inception_5a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 7, 7, 384)    663936      conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 7, 7, 128)    153728      conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 7, 7, 128)    106624      max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 128)    65664       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 4, 4, 128)    67712       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "inception_5b (Concatenate)      (None, 7, 7, 1024)   0           conv2d_51[0][0]                  \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "                                                                 conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2048)         0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2048)         0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_5_3x3/1 (GlobalAverage (None, 1024)         0           inception_5b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         2098176     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         2098176     flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1024)         0           avg_pool_5_3x3/1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 10)           10250       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "auxilliary_output_1 (Dense)     (None, 10)           10250       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "auxilliary_output_2 (Dense)     (None, 10)           10250       dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 10,334,030\n",
            "Trainable params: 10,334,030\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFb5EcKrujoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100\n",
        "initial_lrate = 0.0084 #the one we found optimal for AlexNet\n",
        "\n",
        "def decay(epoch, steps=100):\n",
        "    initial_lrate = 0.01\n",
        "    drop = 0.96\n",
        "    epochs_drop = 8\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "\n",
        "sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\n",
        "\n",
        "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
        "\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "getXKMuGu58m",
        "colab_type": "code",
        "outputId": "9eb83d6c-72d5-4e49-bb37-5e7d81639867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, [y_train, y_train, y_train], validation_data=(X_test, [y_test, y_test, y_test]), epochs=epochs, batch_size=256, callbacks=[lr_sc])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3000 samples, validate on 600 samples\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.01.\n",
            "3000/3000 [==============================] - 9s 3ms/step - loss: 2.6167 - output_loss: 1.6375 - auxilliary_output_1_loss: 1.6253 - auxilliary_output_2_loss: 1.6387 - output_acc: 0.3140 - auxilliary_output_1_acc: 0.3293 - auxilliary_output_2_acc: 0.3177 - val_loss: 2.5945 - val_output_loss: 1.6320 - val_auxilliary_output_1_loss: 1.5999 - val_auxilliary_output_2_loss: 1.6084 - val_output_acc: 0.3000 - val_auxilliary_output_1_acc: 0.3283 - val_auxilliary_output_2_acc: 0.3217\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.01.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.5828 - output_loss: 1.6135 - auxilliary_output_1_loss: 1.6061 - auxilliary_output_2_loss: 1.6248 - output_acc: 0.3250 - auxilliary_output_1_acc: 0.3350 - auxilliary_output_2_acc: 0.3130 - val_loss: 2.5755 - val_output_loss: 1.6177 - val_auxilliary_output_1_loss: 1.5962 - val_auxilliary_output_2_loss: 1.5964 - val_output_acc: 0.3267 - val_auxilliary_output_1_acc: 0.3467 - val_auxilliary_output_2_acc: 0.3433\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.01.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.5497 - output_loss: 1.5958 - auxilliary_output_1_loss: 1.5767 - auxilliary_output_2_loss: 1.6028 - output_acc: 0.3297 - auxilliary_output_1_acc: 0.3483 - auxilliary_output_2_acc: 0.3263 - val_loss: 2.5759 - val_output_loss: 1.6267 - val_auxilliary_output_1_loss: 1.5719 - val_auxilliary_output_2_loss: 1.5922 - val_output_acc: 0.3417 - val_auxilliary_output_1_acc: 0.3633 - val_auxilliary_output_2_acc: 0.3583\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.01.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.5980 - output_loss: 1.6312 - auxilliary_output_1_loss: 1.6021 - auxilliary_output_2_loss: 1.6202 - output_acc: 0.3150 - auxilliary_output_1_acc: 0.3440 - auxilliary_output_2_acc: 0.3383 - val_loss: 2.5720 - val_output_loss: 1.6344 - val_auxilliary_output_1_loss: 1.5505 - val_auxilliary_output_2_loss: 1.5747 - val_output_acc: 0.3433 - val_auxilliary_output_1_acc: 0.3400 - val_auxilliary_output_2_acc: 0.3733\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.01.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.5180 - output_loss: 1.5766 - auxilliary_output_1_loss: 1.5621 - auxilliary_output_2_loss: 1.5760 - output_acc: 0.3500 - auxilliary_output_1_acc: 0.3567 - auxilliary_output_2_acc: 0.3617 - val_loss: 2.4643 - val_output_loss: 1.5500 - val_auxilliary_output_1_loss: 1.5191 - val_auxilliary_output_2_loss: 1.5284 - val_output_acc: 0.3733 - val_auxilliary_output_1_acc: 0.3750 - val_auxilliary_output_2_acc: 0.3750\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.01.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.4284 - output_loss: 1.5187 - auxilliary_output_1_loss: 1.5086 - auxilliary_output_2_loss: 1.5238 - output_acc: 0.3807 - auxilliary_output_1_acc: 0.3810 - auxilliary_output_2_acc: 0.3827 - val_loss: 2.4669 - val_output_loss: 1.5550 - val_auxilliary_output_1_loss: 1.5087 - val_auxilliary_output_2_loss: 1.5307 - val_output_acc: 0.3367 - val_auxilliary_output_1_acc: 0.3833 - val_auxilliary_output_2_acc: 0.3583\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.01.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.6218 - output_loss: 1.6558 - auxilliary_output_1_loss: 1.6131 - auxilliary_output_2_loss: 1.6068 - output_acc: 0.3223 - auxilliary_output_1_acc: 0.3530 - auxilliary_output_2_acc: 0.3450 - val_loss: 2.8617 - val_output_loss: 1.8462 - val_auxilliary_output_1_loss: 1.7334 - val_auxilliary_output_2_loss: 1.6519 - val_output_acc: 0.2667 - val_auxilliary_output_1_acc: 0.2800 - val_auxilliary_output_2_acc: 0.3167\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.6392 - output_loss: 1.6804 - auxilliary_output_1_loss: 1.5928 - auxilliary_output_2_loss: 1.6030 - output_acc: 0.2900 - auxilliary_output_1_acc: 0.3473 - auxilliary_output_2_acc: 0.3350 - val_loss: 2.5368 - val_output_loss: 1.6220 - val_auxilliary_output_1_loss: 1.5151 - val_auxilliary_output_2_loss: 1.5342 - val_output_acc: 0.3267 - val_auxilliary_output_1_acc: 0.3867 - val_auxilliary_output_2_acc: 0.3867\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.4794 - output_loss: 1.5766 - auxilliary_output_1_loss: 1.4866 - auxilliary_output_2_loss: 1.5225 - output_acc: 0.3407 - auxilliary_output_1_acc: 0.3950 - auxilliary_output_2_acc: 0.3780 - val_loss: 2.4465 - val_output_loss: 1.5605 - val_auxilliary_output_1_loss: 1.4700 - val_auxilliary_output_2_loss: 1.4834 - val_output_acc: 0.3467 - val_auxilliary_output_1_acc: 0.4083 - val_auxilliary_output_2_acc: 0.3817\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.4555 - output_loss: 1.5535 - auxilliary_output_1_loss: 1.4835 - auxilliary_output_2_loss: 1.5232 - output_acc: 0.3580 - auxilliary_output_1_acc: 0.3993 - auxilliary_output_2_acc: 0.3927 - val_loss: 2.5957 - val_output_loss: 1.6936 - val_auxilliary_output_1_loss: 1.4655 - val_auxilliary_output_2_loss: 1.5415 - val_output_acc: 0.3133 - val_auxilliary_output_1_acc: 0.4183 - val_auxilliary_output_2_acc: 0.3850\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.4188 - output_loss: 1.5347 - auxilliary_output_1_loss: 1.4517 - auxilliary_output_2_loss: 1.4950 - output_acc: 0.3750 - auxilliary_output_1_acc: 0.4177 - auxilliary_output_2_acc: 0.4150 - val_loss: 2.3623 - val_output_loss: 1.4903 - val_auxilliary_output_1_loss: 1.4522 - val_auxilliary_output_2_loss: 1.4543 - val_output_acc: 0.3767 - val_auxilliary_output_1_acc: 0.4300 - val_auxilliary_output_2_acc: 0.4167\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.3525 - output_loss: 1.4944 - auxilliary_output_1_loss: 1.4131 - auxilliary_output_2_loss: 1.4472 - output_acc: 0.3807 - auxilliary_output_1_acc: 0.4303 - auxilliary_output_2_acc: 0.4237 - val_loss: 2.3657 - val_output_loss: 1.5017 - val_auxilliary_output_1_loss: 1.4325 - val_auxilliary_output_2_loss: 1.4474 - val_output_acc: 0.3900 - val_auxilliary_output_1_acc: 0.4383 - val_auxilliary_output_2_acc: 0.4100\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.3085 - output_loss: 1.4604 - auxilliary_output_1_loss: 1.3983 - auxilliary_output_2_loss: 1.4288 - output_acc: 0.4057 - auxilliary_output_1_acc: 0.4457 - auxilliary_output_2_acc: 0.4253 - val_loss: 2.3303 - val_output_loss: 1.4591 - val_auxilliary_output_1_loss: 1.4485 - val_auxilliary_output_2_loss: 1.4554 - val_output_acc: 0.4050 - val_auxilliary_output_1_acc: 0.4400 - val_auxilliary_output_2_acc: 0.4100\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.2565 - output_loss: 1.4176 - auxilliary_output_1_loss: 1.3948 - auxilliary_output_2_loss: 1.4016 - output_acc: 0.4237 - auxilliary_output_1_acc: 0.4437 - auxilliary_output_2_acc: 0.4350 - val_loss: 2.2491 - val_output_loss: 1.4222 - val_auxilliary_output_1_loss: 1.3839 - val_auxilliary_output_2_loss: 1.3724 - val_output_acc: 0.4317 - val_auxilliary_output_1_acc: 0.4783 - val_auxilliary_output_2_acc: 0.4617\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0096.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.2363 - output_loss: 1.4076 - auxilliary_output_1_loss: 1.3681 - auxilliary_output_2_loss: 1.3941 - output_acc: 0.4230 - auxilliary_output_1_acc: 0.4553 - auxilliary_output_2_acc: 0.4373 - val_loss: 2.2458 - val_output_loss: 1.4270 - val_auxilliary_output_1_loss: 1.3672 - val_auxilliary_output_2_loss: 1.3619 - val_output_acc: 0.4317 - val_auxilliary_output_1_acc: 0.4817 - val_auxilliary_output_2_acc: 0.4867\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.2183 - output_loss: 1.4001 - auxilliary_output_1_loss: 1.3579 - auxilliary_output_2_loss: 1.3695 - output_acc: 0.4247 - auxilliary_output_1_acc: 0.4583 - auxilliary_output_2_acc: 0.4637 - val_loss: 2.2061 - val_output_loss: 1.3843 - val_auxilliary_output_1_loss: 1.3705 - val_auxilliary_output_2_loss: 1.3690 - val_output_acc: 0.4433 - val_auxilliary_output_1_acc: 0.4733 - val_auxilliary_output_2_acc: 0.4717\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.1707 - output_loss: 1.3596 - auxilliary_output_1_loss: 1.3553 - auxilliary_output_2_loss: 1.3484 - output_acc: 0.4470 - auxilliary_output_1_acc: 0.4597 - auxilliary_output_2_acc: 0.4647 - val_loss: 2.2227 - val_output_loss: 1.3965 - val_auxilliary_output_1_loss: 1.3806 - val_auxilliary_output_2_loss: 1.3733 - val_output_acc: 0.4500 - val_auxilliary_output_1_acc: 0.4700 - val_auxilliary_output_2_acc: 0.4650\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.2068 - output_loss: 1.3903 - auxilliary_output_1_loss: 1.3476 - auxilliary_output_2_loss: 1.3739 - output_acc: 0.4307 - auxilliary_output_1_acc: 0.4637 - auxilliary_output_2_acc: 0.4433 - val_loss: 2.2563 - val_output_loss: 1.4278 - val_auxilliary_output_1_loss: 1.3791 - val_auxilliary_output_2_loss: 1.3827 - val_output_acc: 0.4367 - val_auxilliary_output_1_acc: 0.4633 - val_auxilliary_output_2_acc: 0.4667\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.1501 - output_loss: 1.3498 - auxilliary_output_1_loss: 1.3324 - auxilliary_output_2_loss: 1.3353 - output_acc: 0.4477 - auxilliary_output_1_acc: 0.4673 - auxilliary_output_2_acc: 0.4803 - val_loss: 2.1389 - val_output_loss: 1.3407 - val_auxilliary_output_1_loss: 1.3316 - val_auxilliary_output_2_loss: 1.3292 - val_output_acc: 0.4750 - val_auxilliary_output_1_acc: 0.4867 - val_auxilliary_output_2_acc: 0.4850\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.0941 - output_loss: 1.3131 - auxilliary_output_1_loss: 1.2940 - auxilliary_output_2_loss: 1.3095 - output_acc: 0.4820 - auxilliary_output_1_acc: 0.4910 - auxilliary_output_2_acc: 0.4843 - val_loss: 2.0967 - val_output_loss: 1.3145 - val_auxilliary_output_1_loss: 1.3184 - val_auxilliary_output_2_loss: 1.2890 - val_output_acc: 0.4933 - val_auxilliary_output_1_acc: 0.5083 - val_auxilliary_output_2_acc: 0.5250\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.1110 - output_loss: 1.3362 - auxilliary_output_1_loss: 1.2999 - auxilliary_output_2_loss: 1.2830 - output_acc: 0.4457 - auxilliary_output_1_acc: 0.4803 - auxilliary_output_2_acc: 0.4967 - val_loss: 2.0834 - val_output_loss: 1.3120 - val_auxilliary_output_1_loss: 1.2917 - val_auxilliary_output_2_loss: 1.2796 - val_output_acc: 0.5000 - val_auxilliary_output_1_acc: 0.5050 - val_auxilliary_output_2_acc: 0.5167\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.0318 - output_loss: 1.2735 - auxilliary_output_1_loss: 1.2821 - auxilliary_output_2_loss: 1.2458 - output_acc: 0.4847 - auxilliary_output_1_acc: 0.4900 - auxilliary_output_2_acc: 0.5110 - val_loss: 2.0454 - val_output_loss: 1.2823 - val_auxilliary_output_1_loss: 1.2844 - val_auxilliary_output_2_loss: 1.2594 - val_output_acc: 0.5050 - val_auxilliary_output_1_acc: 0.5200 - val_auxilliary_output_2_acc: 0.5233\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.009216.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.9851 - output_loss: 1.2362 - auxilliary_output_1_loss: 1.2536 - auxilliary_output_2_loss: 1.2425 - output_acc: 0.4980 - auxilliary_output_1_acc: 0.4947 - auxilliary_output_2_acc: 0.5103 - val_loss: 2.0213 - val_output_loss: 1.2603 - val_auxilliary_output_1_loss: 1.2849 - val_auxilliary_output_2_loss: 1.2518 - val_output_acc: 0.5050 - val_auxilliary_output_1_acc: 0.5083 - val_auxilliary_output_2_acc: 0.5100\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.9753 - output_loss: 1.2337 - auxilliary_output_1_loss: 1.2452 - auxilliary_output_2_loss: 1.2268 - output_acc: 0.5010 - auxilliary_output_1_acc: 0.5047 - auxilliary_output_2_acc: 0.5143 - val_loss: 2.0031 - val_output_loss: 1.2489 - val_auxilliary_output_1_loss: 1.2780 - val_auxilliary_output_2_loss: 1.2359 - val_output_acc: 0.4850 - val_auxilliary_output_1_acc: 0.5017 - val_auxilliary_output_2_acc: 0.5117\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.9772 - output_loss: 1.2406 - auxilliary_output_1_loss: 1.2422 - auxilliary_output_2_loss: 1.2129 - output_acc: 0.4977 - auxilliary_output_1_acc: 0.5067 - auxilliary_output_2_acc: 0.5047 - val_loss: 2.0058 - val_output_loss: 1.2577 - val_auxilliary_output_1_loss: 1.2657 - val_auxilliary_output_2_loss: 1.2278 - val_output_acc: 0.5033 - val_auxilliary_output_1_acc: 0.5167 - val_auxilliary_output_2_acc: 0.5200\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.9578 - output_loss: 1.2249 - auxilliary_output_1_loss: 1.2357 - auxilliary_output_2_loss: 1.2073 - output_acc: 0.4967 - auxilliary_output_1_acc: 0.5107 - auxilliary_output_2_acc: 0.5200 - val_loss: 2.1132 - val_output_loss: 1.3316 - val_auxilliary_output_1_loss: 1.3052 - val_auxilliary_output_2_loss: 1.3001 - val_output_acc: 0.4550 - val_auxilliary_output_1_acc: 0.4950 - val_auxilliary_output_2_acc: 0.4700\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.9419 - output_loss: 1.2127 - auxilliary_output_1_loss: 1.2275 - auxilliary_output_2_loss: 1.2030 - output_acc: 0.5163 - auxilliary_output_1_acc: 0.5123 - auxilliary_output_2_acc: 0.5267 - val_loss: 1.9854 - val_output_loss: 1.2433 - val_auxilliary_output_1_loss: 1.2507 - val_auxilliary_output_2_loss: 1.2228 - val_output_acc: 0.5117 - val_auxilliary_output_1_acc: 0.5250 - val_auxilliary_output_2_acc: 0.5350\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.9129 - output_loss: 1.1966 - auxilliary_output_1_loss: 1.2054 - auxilliary_output_2_loss: 1.1824 - output_acc: 0.5133 - auxilliary_output_1_acc: 0.5233 - auxilliary_output_2_acc: 0.5150 - val_loss: 2.0040 - val_output_loss: 1.2424 - val_auxilliary_output_1_loss: 1.2749 - val_auxilliary_output_2_loss: 1.2636 - val_output_acc: 0.4967 - val_auxilliary_output_1_acc: 0.4917 - val_auxilliary_output_2_acc: 0.4900\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.9095 - output_loss: 1.1949 - auxilliary_output_1_loss: 1.2034 - auxilliary_output_2_loss: 1.1786 - output_acc: 0.5223 - auxilliary_output_1_acc: 0.5203 - auxilliary_output_2_acc: 0.5303 - val_loss: 1.9584 - val_output_loss: 1.2321 - val_auxilliary_output_1_loss: 1.2187 - val_auxilliary_output_2_loss: 1.2021 - val_output_acc: 0.5083 - val_auxilliary_output_1_acc: 0.5217 - val_auxilliary_output_2_acc: 0.5333\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.8520 - output_loss: 1.1594 - auxilliary_output_1_loss: 1.1613 - auxilliary_output_2_loss: 1.1472 - output_acc: 0.5353 - auxilliary_output_1_acc: 0.5323 - auxilliary_output_2_acc: 0.5367 - val_loss: 1.9398 - val_output_loss: 1.2126 - val_auxilliary_output_1_loss: 1.2264 - val_auxilliary_output_2_loss: 1.1976 - val_output_acc: 0.5017 - val_auxilliary_output_1_acc: 0.5150 - val_auxilliary_output_2_acc: 0.5250\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.008847359999999999.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.8553 - output_loss: 1.1574 - auxilliary_output_1_loss: 1.1775 - auxilliary_output_2_loss: 1.1486 - output_acc: 0.5263 - auxilliary_output_1_acc: 0.5380 - auxilliary_output_2_acc: 0.5360 - val_loss: 1.9367 - val_output_loss: 1.2186 - val_auxilliary_output_1_loss: 1.2135 - val_auxilliary_output_2_loss: 1.1803 - val_output_acc: 0.5250 - val_auxilliary_output_1_acc: 0.5317 - val_auxilliary_output_2_acc: 0.5467\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.8254 - output_loss: 1.1381 - auxilliary_output_1_loss: 1.1537 - auxilliary_output_2_loss: 1.1374 - output_acc: 0.5293 - auxilliary_output_1_acc: 0.5420 - auxilliary_output_2_acc: 0.5413 - val_loss: 1.8886 - val_output_loss: 1.1882 - val_auxilliary_output_1_loss: 1.1898 - val_auxilliary_output_2_loss: 1.1449 - val_output_acc: 0.5183 - val_auxilliary_output_1_acc: 0.5300 - val_auxilliary_output_2_acc: 0.5417\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.8143 - output_loss: 1.1339 - auxilliary_output_1_loss: 1.1416 - auxilliary_output_2_loss: 1.1264 - output_acc: 0.5400 - auxilliary_output_1_acc: 0.5383 - auxilliary_output_2_acc: 0.5480 - val_loss: 1.9709 - val_output_loss: 1.2366 - val_auxilliary_output_1_loss: 1.2347 - val_auxilliary_output_2_loss: 1.2128 - val_output_acc: 0.5100 - val_auxilliary_output_1_acc: 0.5283 - val_auxilliary_output_2_acc: 0.5300\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.8172 - output_loss: 1.1414 - auxilliary_output_1_loss: 1.1449 - auxilliary_output_2_loss: 1.1080 - output_acc: 0.5473 - auxilliary_output_1_acc: 0.5470 - auxilliary_output_2_acc: 0.5680 - val_loss: 1.9136 - val_output_loss: 1.2082 - val_auxilliary_output_1_loss: 1.1932 - val_auxilliary_output_2_loss: 1.1584 - val_output_acc: 0.4983 - val_auxilliary_output_1_acc: 0.5333 - val_auxilliary_output_2_acc: 0.5283\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.7721 - output_loss: 1.0989 - auxilliary_output_1_loss: 1.1430 - auxilliary_output_2_loss: 1.1010 - output_acc: 0.5610 - auxilliary_output_1_acc: 0.5383 - auxilliary_output_2_acc: 0.5650 - val_loss: 1.8922 - val_output_loss: 1.1965 - val_auxilliary_output_1_loss: 1.1726 - val_auxilliary_output_2_loss: 1.1465 - val_output_acc: 0.5150 - val_auxilliary_output_1_acc: 0.5433 - val_auxilliary_output_2_acc: 0.5300\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.7605 - output_loss: 1.1009 - auxilliary_output_1_loss: 1.1107 - auxilliary_output_2_loss: 1.0880 - output_acc: 0.5437 - auxilliary_output_1_acc: 0.5557 - auxilliary_output_2_acc: 0.5693 - val_loss: 1.8318 - val_output_loss: 1.1474 - val_auxilliary_output_1_loss: 1.1659 - val_auxilliary_output_2_loss: 1.1155 - val_output_acc: 0.5283 - val_auxilliary_output_1_acc: 0.5450 - val_auxilliary_output_2_acc: 0.5533\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.7206 - output_loss: 1.0670 - auxilliary_output_1_loss: 1.1101 - auxilliary_output_2_loss: 1.0682 - output_acc: 0.5800 - auxilliary_output_1_acc: 0.5560 - auxilliary_output_2_acc: 0.5723 - val_loss: 1.9151 - val_output_loss: 1.2049 - val_auxilliary_output_1_loss: 1.1799 - val_auxilliary_output_2_loss: 1.1874 - val_output_acc: 0.4983 - val_auxilliary_output_1_acc: 0.5350 - val_auxilliary_output_2_acc: 0.5200\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.7185 - output_loss: 1.0688 - auxilliary_output_1_loss: 1.0890 - auxilliary_output_2_loss: 1.0767 - output_acc: 0.5660 - auxilliary_output_1_acc: 0.5727 - auxilliary_output_2_acc: 0.5700 - val_loss: 1.9424 - val_output_loss: 1.2323 - val_auxilliary_output_1_loss: 1.1863 - val_auxilliary_output_2_loss: 1.1807 - val_output_acc: 0.5100 - val_auxilliary_output_1_acc: 0.5500 - val_auxilliary_output_2_acc: 0.5667\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.008493465599999998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.7296 - output_loss: 1.0819 - auxilliary_output_1_loss: 1.0889 - auxilliary_output_2_loss: 1.0701 - output_acc: 0.5573 - auxilliary_output_1_acc: 0.5687 - auxilliary_output_2_acc: 0.5713 - val_loss: 1.9642 - val_output_loss: 1.2510 - val_auxilliary_output_1_loss: 1.1860 - val_auxilliary_output_2_loss: 1.1915 - val_output_acc: 0.5067 - val_auxilliary_output_1_acc: 0.5267 - val_auxilliary_output_2_acc: 0.5300\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.7941 - output_loss: 1.1348 - auxilliary_output_1_loss: 1.1107 - auxilliary_output_2_loss: 1.0869 - output_acc: 0.5323 - auxilliary_output_1_acc: 0.5660 - auxilliary_output_2_acc: 0.5737 - val_loss: 1.8321 - val_output_loss: 1.1389 - val_auxilliary_output_1_loss: 1.1737 - val_auxilliary_output_2_loss: 1.1370 - val_output_acc: 0.5417 - val_auxilliary_output_1_acc: 0.5350 - val_auxilliary_output_2_acc: 0.5617\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.7122 - output_loss: 1.0587 - auxilliary_output_1_loss: 1.1078 - auxilliary_output_2_loss: 1.0704 - output_acc: 0.5747 - auxilliary_output_1_acc: 0.5560 - auxilliary_output_2_acc: 0.5743 - val_loss: 1.7847 - val_output_loss: 1.1144 - val_auxilliary_output_1_loss: 1.1446 - val_auxilliary_output_2_loss: 1.0899 - val_output_acc: 0.5433 - val_auxilliary_output_1_acc: 0.5467 - val_auxilliary_output_2_acc: 0.5633\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.6387 - output_loss: 1.0132 - auxilliary_output_1_loss: 1.0587 - auxilliary_output_2_loss: 1.0262 - output_acc: 0.5947 - auxilliary_output_1_acc: 0.5797 - auxilliary_output_2_acc: 0.5893 - val_loss: 1.8511 - val_output_loss: 1.1709 - val_auxilliary_output_1_loss: 1.1387 - val_auxilliary_output_2_loss: 1.1286 - val_output_acc: 0.5067 - val_auxilliary_output_1_acc: 0.5467 - val_auxilliary_output_2_acc: 0.5283\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.6704 - output_loss: 1.0407 - auxilliary_output_1_loss: 1.0576 - auxilliary_output_2_loss: 1.0415 - output_acc: 0.5757 - auxilliary_output_1_acc: 0.5823 - auxilliary_output_2_acc: 0.5800 - val_loss: 1.8107 - val_output_loss: 1.1364 - val_auxilliary_output_1_loss: 1.1539 - val_auxilliary_output_2_loss: 1.0936 - val_output_acc: 0.5167 - val_auxilliary_output_1_acc: 0.5633 - val_auxilliary_output_2_acc: 0.5550\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.6597 - output_loss: 1.0361 - auxilliary_output_1_loss: 1.0545 - auxilliary_output_2_loss: 1.0243 - output_acc: 0.5800 - auxilliary_output_1_acc: 0.5787 - auxilliary_output_2_acc: 0.5867 - val_loss: 1.8972 - val_output_loss: 1.2041 - val_auxilliary_output_1_loss: 1.1670 - val_auxilliary_output_2_loss: 1.1433 - val_output_acc: 0.5317 - val_auxilliary_output_1_acc: 0.5467 - val_auxilliary_output_2_acc: 0.5450\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.6235 - output_loss: 1.0015 - auxilliary_output_1_loss: 1.0658 - auxilliary_output_2_loss: 1.0076 - output_acc: 0.5973 - auxilliary_output_1_acc: 0.5793 - auxilliary_output_2_acc: 0.5930 - val_loss: 1.7698 - val_output_loss: 1.1180 - val_auxilliary_output_1_loss: 1.1059 - val_auxilliary_output_2_loss: 1.0665 - val_output_acc: 0.5483 - val_auxilliary_output_1_acc: 0.5750 - val_auxilliary_output_2_acc: 0.5783\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.5994 - output_loss: 0.9857 - auxilliary_output_1_loss: 1.0436 - auxilliary_output_2_loss: 1.0021 - output_acc: 0.5987 - auxilliary_output_1_acc: 0.5813 - auxilliary_output_2_acc: 0.6070 - val_loss: 1.8460 - val_output_loss: 1.1816 - val_auxilliary_output_1_loss: 1.1126 - val_auxilliary_output_2_loss: 1.1020 - val_output_acc: 0.5183 - val_auxilliary_output_1_acc: 0.5567 - val_auxilliary_output_2_acc: 0.5450\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.008153726976.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.6179 - output_loss: 0.9971 - auxilliary_output_1_loss: 1.0382 - auxilliary_output_2_loss: 1.0312 - output_acc: 0.6010 - auxilliary_output_1_acc: 0.5907 - auxilliary_output_2_acc: 0.5960 - val_loss: 1.8467 - val_output_loss: 1.1733 - val_auxilliary_output_1_loss: 1.1542 - val_auxilliary_output_2_loss: 1.0905 - val_output_acc: 0.5217 - val_auxilliary_output_1_acc: 0.5350 - val_auxilliary_output_2_acc: 0.5633\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.6179 - output_loss: 1.0027 - auxilliary_output_1_loss: 1.0340 - auxilliary_output_2_loss: 1.0163 - output_acc: 0.5920 - auxilliary_output_1_acc: 0.5867 - auxilliary_output_2_acc: 0.6017 - val_loss: 1.7990 - val_output_loss: 1.1242 - val_auxilliary_output_1_loss: 1.1597 - val_auxilliary_output_2_loss: 1.0897 - val_output_acc: 0.5500 - val_auxilliary_output_1_acc: 0.5533 - val_auxilliary_output_2_acc: 0.5867\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.5330 - output_loss: 0.9389 - auxilliary_output_1_loss: 1.0100 - auxilliary_output_2_loss: 0.9704 - output_acc: 0.6237 - auxilliary_output_1_acc: 0.5907 - auxilliary_output_2_acc: 0.6123 - val_loss: 1.7127 - val_output_loss: 1.0603 - val_auxilliary_output_1_loss: 1.0910 - val_auxilliary_output_2_loss: 1.0839 - val_output_acc: 0.5617 - val_auxilliary_output_1_acc: 0.5717 - val_auxilliary_output_2_acc: 0.5800\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.5112 - output_loss: 0.9258 - auxilliary_output_1_loss: 0.9956 - auxilliary_output_2_loss: 0.9559 - output_acc: 0.6273 - auxilliary_output_1_acc: 0.6010 - auxilliary_output_2_acc: 0.6140 - val_loss: 1.7509 - val_output_loss: 1.0784 - val_auxilliary_output_1_loss: 1.1387 - val_auxilliary_output_2_loss: 1.1030 - val_output_acc: 0.5567 - val_auxilliary_output_1_acc: 0.5450 - val_auxilliary_output_2_acc: 0.5567\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.5452 - output_loss: 0.9484 - auxilliary_output_1_loss: 1.0191 - auxilliary_output_2_loss: 0.9701 - output_acc: 0.6240 - auxilliary_output_1_acc: 0.5977 - auxilliary_output_2_acc: 0.6163 - val_loss: 1.7132 - val_output_loss: 1.0750 - val_auxilliary_output_1_loss: 1.0779 - val_auxilliary_output_2_loss: 1.0492 - val_output_acc: 0.5650 - val_auxilliary_output_1_acc: 0.5633 - val_auxilliary_output_2_acc: 0.5850\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.5498 - output_loss: 0.9562 - auxilliary_output_1_loss: 1.0103 - auxilliary_output_2_loss: 0.9682 - output_acc: 0.6213 - auxilliary_output_1_acc: 0.6060 - auxilliary_output_2_acc: 0.6153 - val_loss: 1.7422 - val_output_loss: 1.0891 - val_auxilliary_output_1_loss: 1.1016 - val_auxilliary_output_2_loss: 1.0755 - val_output_acc: 0.5800 - val_auxilliary_output_1_acc: 0.5933 - val_auxilliary_output_2_acc: 0.5833\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.5130 - output_loss: 0.9240 - auxilliary_output_1_loss: 1.0030 - auxilliary_output_2_loss: 0.9602 - output_acc: 0.6310 - auxilliary_output_1_acc: 0.6120 - auxilliary_output_2_acc: 0.6177 - val_loss: 1.7392 - val_output_loss: 1.0745 - val_auxilliary_output_1_loss: 1.1251 - val_auxilliary_output_2_loss: 1.0906 - val_output_acc: 0.5550 - val_auxilliary_output_1_acc: 0.5717 - val_auxilliary_output_2_acc: 0.5900\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.4701 - output_loss: 0.8858 - auxilliary_output_1_loss: 1.0006 - auxilliary_output_2_loss: 0.9469 - output_acc: 0.6480 - auxilliary_output_1_acc: 0.5977 - auxilliary_output_2_acc: 0.6240 - val_loss: 1.7689 - val_output_loss: 1.1154 - val_auxilliary_output_1_loss: 1.1108 - val_auxilliary_output_2_loss: 1.0676 - val_output_acc: 0.5650 - val_auxilliary_output_1_acc: 0.5600 - val_auxilliary_output_2_acc: 0.5983\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 0.007827577896959998.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.4386 - output_loss: 0.8712 - auxilliary_output_1_loss: 0.9701 - auxilliary_output_2_loss: 0.9213 - output_acc: 0.6430 - auxilliary_output_1_acc: 0.6237 - auxilliary_output_2_acc: 0.6370 - val_loss: 1.6520 - val_output_loss: 1.0236 - val_auxilliary_output_1_loss: 1.0655 - val_auxilliary_output_2_loss: 1.0289 - val_output_acc: 0.5933 - val_auxilliary_output_1_acc: 0.5733 - val_auxilliary_output_2_acc: 0.6000\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.4040 - output_loss: 0.8451 - auxilliary_output_1_loss: 0.9545 - auxilliary_output_2_loss: 0.9087 - output_acc: 0.6560 - auxilliary_output_1_acc: 0.6270 - auxilliary_output_2_acc: 0.6400 - val_loss: 1.8544 - val_output_loss: 1.1967 - val_auxilliary_output_1_loss: 1.1071 - val_auxilliary_output_2_loss: 1.0853 - val_output_acc: 0.5483 - val_auxilliary_output_1_acc: 0.5567 - val_auxilliary_output_2_acc: 0.5900\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.5188 - output_loss: 0.9342 - auxilliary_output_1_loss: 0.9884 - auxilliary_output_2_loss: 0.9603 - output_acc: 0.6360 - auxilliary_output_1_acc: 0.6093 - auxilliary_output_2_acc: 0.6233 - val_loss: 1.7487 - val_output_loss: 1.0972 - val_auxilliary_output_1_loss: 1.0869 - val_auxilliary_output_2_loss: 1.0847 - val_output_acc: 0.5583 - val_auxilliary_output_1_acc: 0.5783 - val_auxilliary_output_2_acc: 0.5783\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.4682 - output_loss: 0.8947 - auxilliary_output_1_loss: 0.9693 - auxilliary_output_2_loss: 0.9425 - output_acc: 0.6340 - auxilliary_output_1_acc: 0.6123 - auxilliary_output_2_acc: 0.6183 - val_loss: 1.7066 - val_output_loss: 1.0735 - val_auxilliary_output_1_loss: 1.0709 - val_auxilliary_output_2_loss: 1.0397 - val_output_acc: 0.5833 - val_auxilliary_output_1_acc: 0.5700 - val_auxilliary_output_2_acc: 0.5867\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.4455 - output_loss: 0.8800 - auxilliary_output_1_loss: 0.9600 - auxilliary_output_2_loss: 0.9249 - output_acc: 0.6473 - auxilliary_output_1_acc: 0.6220 - auxilliary_output_2_acc: 0.6347 - val_loss: 1.7037 - val_output_loss: 1.0573 - val_auxilliary_output_1_loss: 1.0920 - val_auxilliary_output_2_loss: 1.0626 - val_output_acc: 0.5667 - val_auxilliary_output_1_acc: 0.5917 - val_auxilliary_output_2_acc: 0.5967\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.4556 - output_loss: 0.8848 - auxilliary_output_1_loss: 0.9660 - auxilliary_output_2_loss: 0.9369 - output_acc: 0.6423 - auxilliary_output_1_acc: 0.6190 - auxilliary_output_2_acc: 0.6210 - val_loss: 1.7095 - val_output_loss: 1.0580 - val_auxilliary_output_1_loss: 1.0866 - val_auxilliary_output_2_loss: 1.0851 - val_output_acc: 0.5667 - val_auxilliary_output_1_acc: 0.5900 - val_auxilliary_output_2_acc: 0.5800\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.4147 - output_loss: 0.8563 - auxilliary_output_1_loss: 0.9630 - auxilliary_output_2_loss: 0.8984 - output_acc: 0.6640 - auxilliary_output_1_acc: 0.6217 - auxilliary_output_2_acc: 0.6427 - val_loss: 1.7758 - val_output_loss: 1.1075 - val_auxilliary_output_1_loss: 1.1386 - val_auxilliary_output_2_loss: 1.0890 - val_output_acc: 0.5533 - val_auxilliary_output_1_acc: 0.5650 - val_auxilliary_output_2_acc: 0.5750\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.3755 - output_loss: 0.8278 - auxilliary_output_1_loss: 0.9388 - auxilliary_output_2_loss: 0.8869 - output_acc: 0.6777 - auxilliary_output_1_acc: 0.6230 - auxilliary_output_2_acc: 0.6427 - val_loss: 1.7626 - val_output_loss: 1.1293 - val_auxilliary_output_1_loss: 1.0519 - val_auxilliary_output_2_loss: 1.0592 - val_output_acc: 0.5417 - val_auxilliary_output_1_acc: 0.5683 - val_auxilliary_output_2_acc: 0.5833\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 0.007514474781081598.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.3768 - output_loss: 0.8322 - auxilliary_output_1_loss: 0.9314 - auxilliary_output_2_loss: 0.8839 - output_acc: 0.6700 - auxilliary_output_1_acc: 0.6350 - auxilliary_output_2_acc: 0.6450 - val_loss: 1.7087 - val_output_loss: 1.0826 - val_auxilliary_output_1_loss: 1.0447 - val_auxilliary_output_2_loss: 1.0423 - val_output_acc: 0.5850 - val_auxilliary_output_1_acc: 0.5933 - val_auxilliary_output_2_acc: 0.5917\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.3221 - output_loss: 0.7902 - auxilliary_output_1_loss: 0.9139 - auxilliary_output_2_loss: 0.8591 - output_acc: 0.6880 - auxilliary_output_1_acc: 0.6393 - auxilliary_output_2_acc: 0.6497 - val_loss: 1.7369 - val_output_loss: 1.0896 - val_auxilliary_output_1_loss: 1.0810 - val_auxilliary_output_2_loss: 1.0766 - val_output_acc: 0.5817 - val_auxilliary_output_1_acc: 0.5867 - val_auxilliary_output_2_acc: 0.5967\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.3189 - output_loss: 0.7808 - auxilliary_output_1_loss: 0.9214 - auxilliary_output_2_loss: 0.8724 - output_acc: 0.6910 - auxilliary_output_1_acc: 0.6413 - auxilliary_output_2_acc: 0.6497 - val_loss: 1.6942 - val_output_loss: 1.0549 - val_auxilliary_output_1_loss: 1.0647 - val_auxilliary_output_2_loss: 1.0662 - val_output_acc: 0.5800 - val_auxilliary_output_1_acc: 0.5950 - val_auxilliary_output_2_acc: 0.5833\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.2968 - output_loss: 0.7619 - auxilliary_output_1_loss: 0.9320 - auxilliary_output_2_loss: 0.8511 - output_acc: 0.7033 - auxilliary_output_1_acc: 0.6350 - auxilliary_output_2_acc: 0.6733 - val_loss: 1.6599 - val_output_loss: 1.0335 - val_auxilliary_output_1_loss: 1.0429 - val_auxilliary_output_2_loss: 1.0454 - val_output_acc: 0.5967 - val_auxilliary_output_1_acc: 0.5767 - val_auxilliary_output_2_acc: 0.5967\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.3151 - output_loss: 0.7835 - auxilliary_output_1_loss: 0.9179 - auxilliary_output_2_loss: 0.8543 - output_acc: 0.6913 - auxilliary_output_1_acc: 0.6357 - auxilliary_output_2_acc: 0.6667 - val_loss: 1.6634 - val_output_loss: 1.0429 - val_auxilliary_output_1_loss: 1.0405 - val_auxilliary_output_2_loss: 1.0278 - val_output_acc: 0.5833 - val_auxilliary_output_1_acc: 0.6100 - val_auxilliary_output_2_acc: 0.6133\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.2939 - output_loss: 0.7576 - auxilliary_output_1_loss: 0.9229 - auxilliary_output_2_loss: 0.8649 - output_acc: 0.6950 - auxilliary_output_1_acc: 0.6430 - auxilliary_output_2_acc: 0.6663 - val_loss: 1.6636 - val_output_loss: 1.0307 - val_auxilliary_output_1_loss: 1.0607 - val_auxilliary_output_2_loss: 1.0489 - val_output_acc: 0.5933 - val_auxilliary_output_1_acc: 0.6000 - val_auxilliary_output_2_acc: 0.6183\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.3212 - output_loss: 0.7859 - auxilliary_output_1_loss: 0.9332 - auxilliary_output_2_loss: 0.8510 - output_acc: 0.6937 - auxilliary_output_1_acc: 0.6297 - auxilliary_output_2_acc: 0.6697 - val_loss: 1.7120 - val_output_loss: 1.0598 - val_auxilliary_output_1_loss: 1.0991 - val_auxilliary_output_2_loss: 1.0748 - val_output_acc: 0.5750 - val_auxilliary_output_1_acc: 0.5717 - val_auxilliary_output_2_acc: 0.5933\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.2126 - output_loss: 0.7056 - auxilliary_output_1_loss: 0.8865 - auxilliary_output_2_loss: 0.8035 - output_acc: 0.7127 - auxilliary_output_1_acc: 0.6480 - auxilliary_output_2_acc: 0.6783 - val_loss: 1.7283 - val_output_loss: 1.1005 - val_auxilliary_output_1_loss: 1.0432 - val_auxilliary_output_2_loss: 1.0494 - val_output_acc: 0.5650 - val_auxilliary_output_1_acc: 0.5850 - val_auxilliary_output_2_acc: 0.6033\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 0.007213895789838334.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.1567 - output_loss: 0.6562 - auxilliary_output_1_loss: 0.8618 - auxilliary_output_2_loss: 0.8066 - output_acc: 0.7390 - auxilliary_output_1_acc: 0.6577 - auxilliary_output_2_acc: 0.6770 - val_loss: 1.7389 - val_output_loss: 1.1177 - val_auxilliary_output_1_loss: 1.0292 - val_auxilliary_output_2_loss: 1.0415 - val_output_acc: 0.5867 - val_auxilliary_output_1_acc: 0.5983 - val_auxilliary_output_2_acc: 0.5783\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.1648 - output_loss: 0.6684 - auxilliary_output_1_loss: 0.8667 - auxilliary_output_2_loss: 0.7879 - output_acc: 0.7353 - auxilliary_output_1_acc: 0.6623 - auxilliary_output_2_acc: 0.6943 - val_loss: 1.7910 - val_output_loss: 1.1628 - val_auxilliary_output_1_loss: 1.0287 - val_auxilliary_output_2_loss: 1.0654 - val_output_acc: 0.6017 - val_auxilliary_output_1_acc: 0.6117 - val_auxilliary_output_2_acc: 0.6017\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.3087 - output_loss: 0.7998 - auxilliary_output_1_loss: 0.8741 - auxilliary_output_2_loss: 0.8223 - output_acc: 0.6963 - auxilliary_output_1_acc: 0.6557 - auxilliary_output_2_acc: 0.6803 - val_loss: 1.7228 - val_output_loss: 1.0988 - val_auxilliary_output_1_loss: 1.0449 - val_auxilliary_output_2_loss: 1.0351 - val_output_acc: 0.5700 - val_auxilliary_output_1_acc: 0.6017 - val_auxilliary_output_2_acc: 0.6017\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.2124 - output_loss: 0.7063 - auxilliary_output_1_loss: 0.8774 - auxilliary_output_2_loss: 0.8096 - output_acc: 0.7167 - auxilliary_output_1_acc: 0.6650 - auxilliary_output_2_acc: 0.6823 - val_loss: 1.7069 - val_output_loss: 1.0753 - val_auxilliary_output_1_loss: 1.0388 - val_auxilliary_output_2_loss: 1.0666 - val_output_acc: 0.5917 - val_auxilliary_output_1_acc: 0.5867 - val_auxilliary_output_2_acc: 0.5883\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.1203 - output_loss: 0.6327 - auxilliary_output_1_loss: 0.8573 - auxilliary_output_2_loss: 0.7681 - output_acc: 0.7553 - auxilliary_output_1_acc: 0.6653 - auxilliary_output_2_acc: 0.7007 - val_loss: 1.7025 - val_output_loss: 1.0642 - val_auxilliary_output_1_loss: 1.0594 - val_auxilliary_output_2_loss: 1.0682 - val_output_acc: 0.5933 - val_auxilliary_output_1_acc: 0.5933 - val_auxilliary_output_2_acc: 0.5933\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.1949 - output_loss: 0.6938 - auxilliary_output_1_loss: 0.8698 - auxilliary_output_2_loss: 0.8006 - output_acc: 0.7340 - auxilliary_output_1_acc: 0.6683 - auxilliary_output_2_acc: 0.6980 - val_loss: 1.8102 - val_output_loss: 1.1734 - val_auxilliary_output_1_loss: 1.0839 - val_auxilliary_output_2_loss: 1.0388 - val_output_acc: 0.5633 - val_auxilliary_output_1_acc: 0.5883 - val_auxilliary_output_2_acc: 0.5933\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.2368 - output_loss: 0.7315 - auxilliary_output_1_loss: 0.8653 - auxilliary_output_2_loss: 0.8187 - output_acc: 0.6990 - auxilliary_output_1_acc: 0.6637 - auxilliary_output_2_acc: 0.6727 - val_loss: 1.6970 - val_output_loss: 1.0822 - val_auxilliary_output_1_loss: 1.0268 - val_auxilliary_output_2_loss: 1.0222 - val_output_acc: 0.5950 - val_auxilliary_output_1_acc: 0.6050 - val_auxilliary_output_2_acc: 0.6117\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.0854 - output_loss: 0.6059 - auxilliary_output_1_loss: 0.8387 - auxilliary_output_2_loss: 0.7595 - output_acc: 0.7747 - auxilliary_output_1_acc: 0.6700 - auxilliary_output_2_acc: 0.7073 - val_loss: 1.6928 - val_output_loss: 1.0676 - val_auxilliary_output_1_loss: 1.0356 - val_auxilliary_output_2_loss: 1.0481 - val_output_acc: 0.5983 - val_auxilliary_output_1_acc: 0.6067 - val_auxilliary_output_2_acc: 0.6233\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 0.0069253399582448.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.0082 - output_loss: 0.5471 - auxilliary_output_1_loss: 0.8085 - auxilliary_output_2_loss: 0.7288 - output_acc: 0.7863 - auxilliary_output_1_acc: 0.6893 - auxilliary_output_2_acc: 0.7183 - val_loss: 1.7107 - val_output_loss: 1.0834 - val_auxilliary_output_1_loss: 1.0434 - val_auxilliary_output_2_loss: 1.0478 - val_output_acc: 0.5983 - val_auxilliary_output_1_acc: 0.6000 - val_auxilliary_output_2_acc: 0.6167\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.9733 - output_loss: 0.5217 - auxilliary_output_1_loss: 0.7952 - auxilliary_output_2_loss: 0.7102 - output_acc: 0.7993 - auxilliary_output_1_acc: 0.6910 - auxilliary_output_2_acc: 0.7200 - val_loss: 1.7583 - val_output_loss: 1.1376 - val_auxilliary_output_1_loss: 1.0255 - val_auxilliary_output_2_loss: 1.0437 - val_output_acc: 0.5950 - val_auxilliary_output_1_acc: 0.6033 - val_auxilliary_output_2_acc: 0.6050\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.9872 - output_loss: 0.5351 - auxilliary_output_1_loss: 0.8028 - auxilliary_output_2_loss: 0.7041 - output_acc: 0.7883 - auxilliary_output_1_acc: 0.6903 - auxilliary_output_2_acc: 0.7240 - val_loss: 1.9845 - val_output_loss: 1.3467 - val_auxilliary_output_1_loss: 1.0690 - val_auxilliary_output_2_loss: 1.0571 - val_output_acc: 0.5900 - val_auxilliary_output_1_acc: 0.6050 - val_auxilliary_output_2_acc: 0.6167\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.0100 - output_loss: 0.5558 - auxilliary_output_1_loss: 0.8040 - auxilliary_output_2_loss: 0.7099 - output_acc: 0.7823 - auxilliary_output_1_acc: 0.6820 - auxilliary_output_2_acc: 0.7263 - val_loss: 1.8746 - val_output_loss: 1.2543 - val_auxilliary_output_1_loss: 1.0217 - val_auxilliary_output_2_loss: 1.0462 - val_output_acc: 0.5900 - val_auxilliary_output_1_acc: 0.6067 - val_auxilliary_output_2_acc: 0.6100\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.0916 - output_loss: 0.6354 - auxilliary_output_1_loss: 0.8066 - auxilliary_output_2_loss: 0.7138 - output_acc: 0.7563 - auxilliary_output_1_acc: 0.6900 - auxilliary_output_2_acc: 0.7180 - val_loss: 1.7468 - val_output_loss: 1.1009 - val_auxilliary_output_1_loss: 1.0568 - val_auxilliary_output_2_loss: 1.0962 - val_output_acc: 0.5883 - val_auxilliary_output_1_acc: 0.5800 - val_auxilliary_output_2_acc: 0.5633\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.1656 - output_loss: 0.6688 - auxilliary_output_1_loss: 0.8670 - auxilliary_output_2_loss: 0.7889 - output_acc: 0.7437 - auxilliary_output_1_acc: 0.6587 - auxilliary_output_2_acc: 0.6833 - val_loss: 1.6891 - val_output_loss: 1.0431 - val_auxilliary_output_1_loss: 1.0805 - val_auxilliary_output_2_loss: 1.0730 - val_output_acc: 0.6150 - val_auxilliary_output_1_acc: 0.5683 - val_auxilliary_output_2_acc: 0.5750\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.0028 - output_loss: 0.5303 - auxilliary_output_1_loss: 0.8312 - auxilliary_output_2_loss: 0.7437 - output_acc: 0.8027 - auxilliary_output_1_acc: 0.6850 - auxilliary_output_2_acc: 0.7023 - val_loss: 1.7577 - val_output_loss: 1.1405 - val_auxilliary_output_1_loss: 1.0071 - val_auxilliary_output_2_loss: 1.0501 - val_output_acc: 0.5833 - val_auxilliary_output_1_acc: 0.6083 - val_auxilliary_output_2_acc: 0.6017\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.9867 - output_loss: 0.5465 - auxilliary_output_1_loss: 0.7752 - auxilliary_output_2_loss: 0.6920 - output_acc: 0.7950 - auxilliary_output_1_acc: 0.7023 - auxilliary_output_2_acc: 0.7290 - val_loss: 1.7149 - val_output_loss: 1.1048 - val_auxilliary_output_1_loss: 1.0062 - val_auxilliary_output_2_loss: 1.0275 - val_output_acc: 0.6100 - val_auxilliary_output_1_acc: 0.6100 - val_auxilliary_output_2_acc: 0.6117\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 0.006648326359915008.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.8978 - output_loss: 0.4759 - auxilliary_output_1_loss: 0.7565 - auxilliary_output_2_loss: 0.6500 - output_acc: 0.8137 - auxilliary_output_1_acc: 0.6957 - auxilliary_output_2_acc: 0.7493 - val_loss: 1.7290 - val_output_loss: 1.0902 - val_auxilliary_output_1_loss: 1.0494 - val_auxilliary_output_2_loss: 1.0797 - val_output_acc: 0.6167 - val_auxilliary_output_1_acc: 0.5983 - val_auxilliary_output_2_acc: 0.6050\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.8807 - output_loss: 0.4502 - auxilliary_output_1_loss: 0.7614 - auxilliary_output_2_loss: 0.6733 - output_acc: 0.8263 - auxilliary_output_1_acc: 0.7017 - auxilliary_output_2_acc: 0.7317 - val_loss: 1.7611 - val_output_loss: 1.1392 - val_auxilliary_output_1_loss: 1.0198 - val_auxilliary_output_2_loss: 1.0532 - val_output_acc: 0.6250 - val_auxilliary_output_1_acc: 0.6067 - val_auxilliary_output_2_acc: 0.6067\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.8219 - output_loss: 0.4012 - auxilliary_output_1_loss: 0.7504 - auxilliary_output_2_loss: 0.6519 - output_acc: 0.8460 - auxilliary_output_1_acc: 0.7100 - auxilliary_output_2_acc: 0.7467 - val_loss: 1.9652 - val_output_loss: 1.3284 - val_auxilliary_output_1_loss: 1.0355 - val_auxilliary_output_2_loss: 1.0870 - val_output_acc: 0.5950 - val_auxilliary_output_1_acc: 0.6067 - val_auxilliary_output_2_acc: 0.6033\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 1.0365 - output_loss: 0.5867 - auxilliary_output_1_loss: 0.7897 - auxilliary_output_2_loss: 0.7100 - output_acc: 0.7780 - auxilliary_output_1_acc: 0.6903 - auxilliary_output_2_acc: 0.7193 - val_loss: 1.6770 - val_output_loss: 1.0577 - val_auxilliary_output_1_loss: 1.0215 - val_auxilliary_output_2_loss: 1.0428 - val_output_acc: 0.6100 - val_auxilliary_output_1_acc: 0.6067 - val_auxilliary_output_2_acc: 0.6083\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.9166 - output_loss: 0.4652 - auxilliary_output_1_loss: 0.7954 - auxilliary_output_2_loss: 0.7093 - output_acc: 0.8260 - auxilliary_output_1_acc: 0.6887 - auxilliary_output_2_acc: 0.7270 - val_loss: 1.8817 - val_output_loss: 1.2359 - val_auxilliary_output_1_loss: 1.0563 - val_auxilliary_output_2_loss: 1.0963 - val_output_acc: 0.5850 - val_auxilliary_output_1_acc: 0.5933 - val_auxilliary_output_2_acc: 0.5883\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.9023 - output_loss: 0.4799 - auxilliary_output_1_loss: 0.7422 - auxilliary_output_2_loss: 0.6658 - output_acc: 0.8097 - auxilliary_output_1_acc: 0.7120 - auxilliary_output_2_acc: 0.7477 - val_loss: 2.0326 - val_output_loss: 1.3989 - val_auxilliary_output_1_loss: 1.0101 - val_auxilliary_output_2_loss: 1.1020 - val_output_acc: 0.5633 - val_auxilliary_output_1_acc: 0.5983 - val_auxilliary_output_2_acc: 0.6083\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.7902 - output_loss: 0.3715 - auxilliary_output_1_loss: 0.7481 - auxilliary_output_2_loss: 0.6475 - output_acc: 0.8647 - auxilliary_output_1_acc: 0.7070 - auxilliary_output_2_acc: 0.7480 - val_loss: 1.8130 - val_output_loss: 1.1851 - val_auxilliary_output_1_loss: 1.0190 - val_auxilliary_output_2_loss: 1.0739 - val_output_acc: 0.6167 - val_auxilliary_output_1_acc: 0.6000 - val_auxilliary_output_2_acc: 0.6100\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.8711 - output_loss: 0.4591 - auxilliary_output_1_loss: 0.7299 - auxilliary_output_2_loss: 0.6433 - output_acc: 0.8173 - auxilliary_output_1_acc: 0.7307 - auxilliary_output_2_acc: 0.7497 - val_loss: 2.1247 - val_output_loss: 1.4614 - val_auxilliary_output_1_loss: 1.0488 - val_auxilliary_output_2_loss: 1.1620 - val_output_acc: 0.5550 - val_auxilliary_output_1_acc: 0.6067 - val_auxilliary_output_2_acc: 0.6133\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 0.006382393305518408.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.9193 - output_loss: 0.4957 - auxilliary_output_1_loss: 0.7638 - auxilliary_output_2_loss: 0.6481 - output_acc: 0.8003 - auxilliary_output_1_acc: 0.7040 - auxilliary_output_2_acc: 0.7473 - val_loss: 1.7511 - val_output_loss: 1.1071 - val_auxilliary_output_1_loss: 1.0675 - val_auxilliary_output_2_loss: 1.0791 - val_output_acc: 0.5850 - val_auxilliary_output_1_acc: 0.5950 - val_auxilliary_output_2_acc: 0.5900\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 0.006127097573297671.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.7251 - output_loss: 0.3144 - auxilliary_output_1_loss: 0.7324 - auxilliary_output_2_loss: 0.6365 - output_acc: 0.8907 - auxilliary_output_1_acc: 0.7190 - auxilliary_output_2_acc: 0.7563 - val_loss: 1.8034 - val_output_loss: 1.1590 - val_auxilliary_output_1_loss: 1.0343 - val_auxilliary_output_2_loss: 1.1138 - val_output_acc: 0.6133 - val_auxilliary_output_1_acc: 0.6083 - val_auxilliary_output_2_acc: 0.6333\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 0.006127097573297671.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.6879 - output_loss: 0.2968 - auxilliary_output_1_loss: 0.7210 - auxilliary_output_2_loss: 0.5827 - output_acc: 0.8907 - auxilliary_output_1_acc: 0.7237 - auxilliary_output_2_acc: 0.7733 - val_loss: 1.9518 - val_output_loss: 1.2836 - val_auxilliary_output_1_loss: 1.0820 - val_auxilliary_output_2_loss: 1.1455 - val_output_acc: 0.6133 - val_auxilliary_output_1_acc: 0.6150 - val_auxilliary_output_2_acc: 0.6083\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 0.006127097573297671.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.8482 - output_loss: 0.4399 - auxilliary_output_1_loss: 0.7260 - auxilliary_output_2_loss: 0.6350 - output_acc: 0.8247 - auxilliary_output_1_acc: 0.7120 - auxilliary_output_2_acc: 0.7573 - val_loss: 2.5993 - val_output_loss: 1.9443 - val_auxilliary_output_1_loss: 1.0343 - val_auxilliary_output_2_loss: 1.1491 - val_output_acc: 0.5200 - val_auxilliary_output_1_acc: 0.6167 - val_auxilliary_output_2_acc: 0.6017\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 0.006127097573297671.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.9943 - output_loss: 0.5797 - auxilliary_output_1_loss: 0.7141 - auxilliary_output_2_loss: 0.6677 - output_acc: 0.7673 - auxilliary_output_1_acc: 0.7310 - auxilliary_output_2_acc: 0.7497 - val_loss: 1.7839 - val_output_loss: 1.1375 - val_auxilliary_output_1_loss: 1.0605 - val_auxilliary_output_2_loss: 1.0942 - val_output_acc: 0.6150 - val_auxilliary_output_1_acc: 0.6017 - val_auxilliary_output_2_acc: 0.6083\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 0.006127097573297671.\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 0.7543 - output_loss: 0.3586 - auxilliary_output_1_loss: 0.7003 - auxilliary_output_2_loss: 0.6186 - output_acc: 0.8683 - auxilliary_output_1_acc: 0.7250 - auxilliary_output_2_acc: 0.7650 - val_loss: 1.8712 - val_output_loss: 1.2459 - val_auxilliary_output_1_loss: 1.0134 - val_auxilliary_output_2_loss: 1.0708 - val_output_acc: 0.6033 - val_auxilliary_output_1_acc: 0.6200 - val_auxilliary_output_2_acc: 0.6267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPLPi7llu9q1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "873d5d78-57cc-46c3-de1e-ee11e691de02"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['output_acc'])\n",
        "plt.plot(history.history['val_output_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFWZB2FzjjAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-6FPIUkvIuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}